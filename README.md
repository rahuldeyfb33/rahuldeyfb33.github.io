## Portfolio

I have 4 years of experience in IT Industry, having worked closely with Big Data Technologies-Spark(Core, SQL, Streaming). Developed highly scalable Data Pipelines which are hosted on AWS EMR and on-premise Hadoop clusters. Programmed applications in languages involving Scala, Python and SQL and have good understanding of Hadoop ecosystem i.e. HDFS, Hive, Sqoop and Apache Kakfa. I have Hands-on experience with AWS services including S3, Glacier, Redshift, Athena, Lambda, Step Function, QuickSight, Elastic MapReduce, Glue, CloudWatch, EC2, RDS, IAM. Developed a Central Logging and Monitoring System using ELK stack(Elasticsearch,Logstash,Filebeat and Kibana) to monitor data pipelines across different Hadoop Clusters. Developed a Real Estate web application using Python’s Framework Django. I have been involved in analysis, design, development, testing, implementation and maintenance with timely delivery against set deadlines.

---

## Skills

<p align='left'>
  <img src="https://upload.wikimedia.org/wikipedia/commons/e/ea/Spark-logo-192x100px.png" alt="html" width="90" height="40">
  <img src='https://upload.wikimedia.org/wikipedia/commons/8/85/Scala_logo.png' alt="css" width="110" height="40">
  <img src='https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg' alt="css" width="150" height="40">
   <img src="https://upload.wikimedia.org/wikipedia/commons/8/87/Sql_data_base_with_logo.png" alt="react" width="auto" height="40"/>
   <img src="https://upload.wikimedia.org/wikipedia/commons/f/f4/Elasticsearch_logo.svg" alt="angular" width="120" height="40"/>
   <img src="https://upload.wikimedia.org/wikipedia/commons/9/93/Amazon_Web_Services_Logo.svg" alt="angular" width="100" height="40"/>
   <img src="https://upload.wikimedia.org/wikipedia/commons/a/a8/Microsoft_Azure_Logo.svg" alt="angular" width="100" height="40"/>
</p>

---

## Experience

### **BOSCH**
### Data Engineer | July, 2020 - Present

Key Responsibilities:<br>
• Developing Spark applications in Scala for processing streaming data and loading it to Hive.<br>
• Tuning performance of Spark jobs to complete data processing within desired time interval.<br>
• Generalizing the solution such that the jobs work in different clusters.<br>
• Developing UNIX/python scripts for deployment and scheduling Spark jobs in clusters.<br>
• Rolling out & setting up data pipelines for Production plants.<br>
• Creating and maintaining Databases in MPP SQL query engine Impala.<br>
• Automating response plans for the Incidents raised by the Data Analyst Team.<br>
• Creating a monitoring and alerting system using Elastic stack for critical data pipelines.<br>
• Implementing new methodology to the existing data pipelines to improve performance.<br>
• Conducting regular knowledge sharing sessions on System Architecture for new stakeholders.<br>
• Coordination/Collaboration with central teams for tasks and standards.<br>


### **ATTRA**
### Associate Data Engineer | July, 2017 - June, 2020

Key Responsibilities:<br>
• Designing the process flow of ETL.<br>
• Developing Spark Application for Initial and Incremental Batch Load.<br>
• Collaborating with data science and business teams to improve data models that feed business intelligence tools.<br>
• Develop tools to improve data flows between external system and data warehouse.<br>
• Work closely with all business units and engineering teams to develop strategy for long term data platform architecture.<br>

---

## Education

### **Visvesvaraya Technological University (VTU)**
### B.E. in Electronics and Communcation Engineering (2012- 2016)


---

### INTERESTS

My interest are Sports, Astrophysics, Cryptocurrencies, Music, Movies, Anime and Latest Tech
